#!/bin/bash
#SBATCH --time=24:00:00
#SBATCH --nodes=1 --ntasks-per-node=1 --cpus-per-task=4
#SBATCH --partition=gpu --gres=gpu:1
#SBATCH --mem=48G
#SBATCH --error=../logs/distilbert-ict.%j.err
#SBATCH --out=../logs/distilbert-ict.%j.out

module purge
ml PyTorch/1.7.1-fosscuda-2019b-Python-3.7.4

cd ../src/bert
source /home/cernypro/dev/envs/huggingface_gpu/bin/activate

XDG_RUNTIME_DIR=""
HF_HOME="/data/temporary/huggingface_${SLURM_JOB_ID}"
export HF_HOME

# TRAINPATH='/home/cernypro/dev/source/ml4logs/data/processed/Combined_20210401_uniform_only_target_flat_context/train'
TRAINPATH='/home/cernypro/dev/source/ml4logs/data/processed/Combined_20210401_epochs-3_max-avg_only_target_flat_context'
# TRAINPATH='/home/cernypro/dev/source/ml4logs/data/processed/train-data-HDFS1-cv1-1-time-ordered_Epochs-4_Seed-43'
EVALPATH='/home/cernypro/dev/source/ml4logs/data/processed/val-data-HDFS1-cv1-1-time-ordered_Epochs-1_Seed-43'

# mkdir -p /data/temporary/bert_log_embedding_temp/train
# mkdir -p /data/temporary/bert_log_embedding_temp/eval
# cp -R ${TRAINPATH}/* /data/temporary/bert_log_embedding_temp/train
# cp -R ${EVALPATH}/* /data/temporary/bert_log_embedding_temp/eval

python3 -u ./train_ict_preprocessed_data.py --fp16 \
    --output-encode-dim 100 \
    --train-batch-size 64 \
    --target-max-seq-len 512 \
    --eval-steps 5000 \
    --save-steps 2500 \
    --logging-steps 50 \
    --dataset-name Combined_20210401_max-avg_3_epochs \
    --train-dataset ${TRAINPATH} \
    --eval-dataset ${EVALPATH}
