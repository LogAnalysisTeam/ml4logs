{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "closing-stretch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-edition",
   "metadata": {},
   "source": [
    "# Removing timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-produce",
   "metadata": {},
   "source": [
    "# HDFS1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "twenty-southeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs1_log_path = Path('/home/cernypro/dev/source/ml4logs/data/interim/HDFS1/train-data-HDFS1-cv1-1-time-ordered.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "naked-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hdfs1_lines(lines: List[str]):\n",
    "    HDFS1_TIMESTAMP_PATTERN = re.compile(r'^(\\d+) (\\d+) (\\d+) ')\n",
    "    NULL_CHAR_PATTERN = re.compile('\\x00')\n",
    "    stripped = (NULL_CHAR_PATTERN.sub('', line).strip() for line in lines)\n",
    "    no_timestamps = [HDFS1_TIMESTAMP_PATTERN.sub('', line)+'\\n' for line in stripped if line]\n",
    "    return no_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dutch-granny",
   "metadata": {},
   "outputs": [],
   "source": [
    "with hdfs1_log_path.open(mode='r') as f:\n",
    "    hdfs1_lines = [line for line in f]\n",
    "hdfs1_cleaned = clean_hdfs1_lines(hdfs1_lines)\n",
    "with (hdfs1_log_path.parent / f'no_timestamps_{hdfs1_log_path.stem}.log').open(mode='w') as f:\n",
    "    f.writelines(hdfs1_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sized-oracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "del hdfs1_lines\n",
    "del hdfs1_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-consistency",
   "metadata": {},
   "source": [
    "## HDFS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "revolutionary-engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "logfolder_path = Path('/home/cernypro/dev/source/ml4logs/data/raw/HDFS2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "wrong-programming",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = Path('/home/cernypro/dev/source/ml4logs/data/interim/HDFS2/simple_cleaning')\n",
    "output_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "complete-capital",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lines_remove_timestamps(lines: List[str]) -> List[str]:\n",
    "    HDFS2_TIMESTAMP_PATTERN = re.compile(r'^\\d{4,4}-\\d\\d-\\d\\d \\d\\d:\\d\\d:\\d\\d,\\d{3,3} ')\n",
    "    HDFS2_COMMENT_PATTERN = re.compile(r'^/?\\*+/?$')\n",
    "    NULL_CHAR_PATTERN = re.compile('\\x00')\n",
    "    no_comments = (line for line in lines if HDFS2_COMMENT_PATTERN.match(line) is None)\n",
    "    stripped = (NULL_CHAR_PATTERN.sub('', line).strip() for line in no_comments)\n",
    "    no_timestamps = [HDFS2_TIMESTAMP_PATTERN.sub('', line)+'\\n' for line in stripped if line]\n",
    "    return no_timestamps\n",
    "\n",
    "def clean_lines_only(lines: List[str]) -> List[str]:\n",
    "    HDFS2_TIMESTAMP_PATTERN = re.compile(r'^\\d{4,4}-\\d\\d-\\d\\d \\d\\d:\\d\\d:\\d\\d,\\d{3,3} ')\n",
    "    HDFS2_COMMENT_PATTERN = re.compile(r'^/?\\*+/?$')\n",
    "    NULL_CHAR_PATTERN = re.compile('\\x00')\n",
    "    no_comments = (line for line in lines if HDFS2_COMMENT_PATTERN.match(line) is None)\n",
    "    stripped = (NULL_CHAR_PATTERN.sub('', line).strip() for line in no_comments)\n",
    "    cleaned_with_newlines = [line+'\\n' for line in stripped if line]\n",
    "    return cleaned_with_newlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-medicaid",
   "metadata": {},
   "outputs": [],
   "source": [
    "for logfile_path in logfolder_path.glob('*.log'):\n",
    "    print(logfile_path.stem)\n",
    "    with logfile_path.open(mode='r') as f:\n",
    "        lines = [line for line in f]\n",
    "    cleaned = clean_lines_only(lines)\n",
    "    with (output_folder / logfile_path.name).open(mode='w') as f:\n",
    "        f.writelines(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-filing",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "relative-officer",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_log_path = Path('/home/cernypro/dev/source/ml4logs/data/interim/Spark/concatenated_spark.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "strange-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_spark_lines(lines: List[str]):\n",
    "    SPARK_TIMESTAMP_PATTERN = re.compile(r'\\d\\d/\\d\\d/\\d\\d \\d\\d:\\d\\d:\\d\\d ')\n",
    "    NULL_CHAR_PATTERN = re.compile('\\x00')\n",
    "    stripped = (NULL_CHAR_PATTERN.sub('', line).strip() for line in lines)\n",
    "    no_timestamps = [SPARK_TIMESTAMP_PATTERN.sub('', line)+'\\n' for line in stripped if line]\n",
    "    return no_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "european-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "with spark_log_path.open(mode='r') as f:\n",
    "    lines = [line for line in f]\n",
    "cleaned = clean_spark_lines(lines)\n",
    "with (spark_log_path.parent / 'no_timestamps_spark.log').open(mode='w') as f:\n",
    "    f.writelines(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "obvious-carbon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SLF4J: Class path contains multiple SLF4J bindings.\\n',\n",
       " 'SLF4J: Found binding in [jar:file:/opt/hdfs/nodemanager/usercache/yxsu/filecache/20/spark-assembly-1.4.1-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\\n',\n",
       " 'SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\\n',\n",
       " 'SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\\n',\n",
       " 'SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\\n',\n",
       " 'INFO executor.CoarseGrainedExecutorBackend: Registered signal handlers for [TERM, HUP, INT]\\n',\n",
       " 'INFO spark.SecurityManager: Changing view acls to: yarn,yxsu\\n',\n",
       " 'INFO spark.SecurityManager: Changing modify acls to: yarn,yxsu\\n',\n",
       " 'INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(yarn, yxsu); users with modify permissions: Set(yarn, yxsu)\\n',\n",
       " 'INFO slf4j.Slf4jLogger: Slf4jLogger started\\n',\n",
       " 'INFO Remoting: Starting remoting\\n',\n",
       " 'INFO Remoting: Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@mesos-slave-30:48707]\\n',\n",
       " \"INFO util.Utils: Successfully started service 'driverPropsFetcher' on port 48707.\\n\",\n",
       " 'INFO spark.SecurityManager: Changing view acls to: yarn,yxsu\\n',\n",
       " 'INFO spark.SecurityManager: Changing modify acls to: yarn,yxsu\\n',\n",
       " 'INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(yarn, yxsu); users with modify permissions: Set(yarn, yxsu)\\n',\n",
       " 'INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.\\n',\n",
       " 'INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.\\n',\n",
       " 'INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.\\n',\n",
       " 'INFO slf4j.Slf4jLogger: Slf4jLogger started\\n']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-ireland",
   "metadata": {},
   "source": [
    "## Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "peaceful-support",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_log_path = Path('/home/cernypro/dev/source/ml4logs/data/interim/Hadoop/concatenated_hadoop.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hindu-language",
   "metadata": {},
   "outputs": [],
   "source": [
    "with hadoop_log_path.open(mode='r') as f:\n",
    "    hadoop_lines = [line for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "immune-assets",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hadoop_lines(lines: List[str]):\n",
    "    HADOOP_TIMESTAMP_PATTERN = re.compile(r'\\d{4,4}-\\d\\d-\\d\\d \\d\\d:\\d\\d:\\d\\d,\\d{3,3} ')\n",
    "    NULL_CHAR_PATTERN = re.compile('\\x00')\n",
    "    stripped = (NULL_CHAR_PATTERN.sub('', line).strip() for line in lines)\n",
    "    no_timestamps = [HADOOP_TIMESTAMP_PATTERN.sub('', line)+'\\n' for line in stripped if line]\n",
    "    return no_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "premium-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_cleaned = clean_hadoop_lines(hadoop_lines)\n",
    "with (hadoop_log_path.parent / 'no_timestamps_hadoop.log').open(mode='w') as f:\n",
    "    f.writelines(hadoop_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-spirituality",
   "metadata": {},
   "outputs": [],
   "source": [
    "del hadoop_lines\n",
    "del hadoop_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-layout",
   "metadata": {},
   "source": [
    "## Zookeeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "terminal-imaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "zookeeper_log_path = Path('/home/cernypro/dev/source/ml4logs/data/raw/Zookeeper/Zookeeper.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "labeled-johnson",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zookeeper_log_path.open(mode='r') as f:\n",
    "    zookeeper_lines = [line for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "expired-liabilities",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2015-07-29 17:41:41,536 - INFO  [main:QuorumPeerConfig@101] - Reading configuration from: /etc/zookeeper/conf/zoo.cfg\\n',\n",
       " '2015-07-29 17:41:41,544 - INFO  [main:QuorumPeerConfig@334] - Defaulting to majority quorums\\n',\n",
       " '2015-07-29 17:41:41,555 - INFO  [main:DatadirCleanupManager@78] - autopurge.snapRetainCount set to 3\\n',\n",
       " '2015-07-29 17:41:41,555 - INFO  [main:DatadirCleanupManager@79] - autopurge.purgeInterval set to 0\\n',\n",
       " '2015-07-29 17:41:41,557 - INFO  [main:DatadirCleanupManager@101] - Purge task is not scheduled.\\n',\n",
       " '2015-07-29 17:41:41,579 - INFO  [main:QuorumPeerMain@127] - Starting quorum peer\\n',\n",
       " '2015-07-29 17:41:41,609 - INFO  [main:NIOServerCnxnFactory@94] - binding to port 0.0.0.0/0.0.0.0:2181\\n',\n",
       " '2015-07-29 17:41:41,648 - INFO  [main:QuorumPeer@913] - tickTime set to 2000\\n',\n",
       " '2015-07-29 17:41:41,649 - INFO  [main:QuorumPeer@933] - minSessionTimeout set to -1\\n',\n",
       " '2015-07-29 17:41:41,649 - INFO  [main:QuorumPeer@944] - maxSessionTimeout set to -1\\n']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zookeeper_lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "angry-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_zookeeper_lines(lines: List[str]):\n",
    "    ZOOKEEPER_TIMESTAMP_PATTERN = re.compile(r'\\d{4,4}-\\d\\d-\\d\\d \\d\\d:\\d\\d:\\d\\d,\\d{3,3} - ')\n",
    "    NULL_CHAR_PATTERN = re.compile('\\x00')\n",
    "    stripped = (NULL_CHAR_PATTERN.sub('', line).strip() for line in lines)\n",
    "    no_timestamps = [ZOOKEEPER_TIMESTAMP_PATTERN.sub('', line)+'\\n' for line in stripped if line]\n",
    "    return no_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "neutral-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "zookeeper_cleaned = clean_zookeeper_lines(zookeeper_lines)\n",
    "zookeeper_output_file_path = zookeeper_log_path.parent.parent.parent / 'interim' / 'Zookeeper' / 'no_timestamps_zookeeper.log'\n",
    "zookeeper_output_file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with (zookeeper_output_file_path).open(mode='w') as f:\n",
    "    f.writelines(zookeeper_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cloudy-illness",
   "metadata": {},
   "outputs": [],
   "source": [
    "del zookeeper_lines\n",
    "del zookeeper_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-record",
   "metadata": {},
   "source": [
    "## BGL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "tribal-collection",
   "metadata": {},
   "outputs": [],
   "source": [
    "bgl_log_path = Path('/home/cernypro/dev/source/ml4logs/data/raw/BGL/BGL.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "finished-scott",
   "metadata": {},
   "outputs": [],
   "source": [
    "with bgl_log_path.open(mode='r') as f:\n",
    "    bgl_lines = [line for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "operational-morning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['- 1117838570 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.50.363779 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\\n',\n",
       " '- 1117838570 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.50.527847 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\\n',\n",
       " '- 1117838570 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.50.675872 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\\n',\n",
       " '- 1117838570 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.50.823719 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\\n',\n",
       " '- 1117838570 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.50.982731 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\\n',\n",
       " '- 1117838571 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.51.131467 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\\n',\n",
       " '- 1117838571 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.51.293532 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\\n',\n",
       " '- 1117838571 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.51.428563 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\\n',\n",
       " '- 1117838571 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.51.601412 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\\n',\n",
       " '- 1117838571 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.51.749199 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\\n']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bgl_lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "restricted-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_bgl_lines(lines: List[str]):\n",
    "    BGL_TIMESTAMP_PATTERN = re.compile(r'\\d+ \\d{4,4}.\\d\\d.\\d\\d (?P<someID>[-:\\w]+) \\d{4,4}-\\d\\d-\\d\\d-\\d\\d\\.\\d\\d\\.\\d\\d\\.\\d+ ')\n",
    "    NULL_CHAR_PATTERN = re.compile('\\x00')\n",
    "    stripped = (NULL_CHAR_PATTERN.sub('', line).strip() for line in lines)\n",
    "    no_timestamps = [BGL_TIMESTAMP_PATTERN.sub('', line)+'\\n' for line in stripped if line]  # could sub with '\\g<someID> ', but it appears that ID is duplicated when present, so we can leave one out\n",
    "    return no_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "pursuant-kelly",
   "metadata": {},
   "outputs": [],
   "source": [
    "bgl_cleaned = clean_bgl_lines(bgl_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "atlantic-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": [
    "bgl_output_file_path = bgl_log_path.parent.parent.parent / 'interim' / 'BGL' / 'no_timestamps_bgl.log'\n",
    "bgl_output_file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with (bgl_output_file_path).open(mode='w') as f:\n",
    "    f.writelines(bgl_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-coating",
   "metadata": {},
   "outputs": [],
   "source": [
    "del bgl_lines\n",
    "del bgl_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-welding",
   "metadata": {},
   "source": [
    "# Creating combined dataset\n",
    "Datasets are already pretokenized and chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unauthorized-pitch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "south-renaissance",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_dataset_paths = {\n",
    "    'train-data-HDFS1-cv1-1-time-ordered': '/home/cernypro/dev/source/ml4logs/data/interim/HDFS1/no_timestamps_train-data-HDFS1-cv1-1-time-ordered/chunked_size_10_tokens_text',\n",
    "    'HDFS2-secondarynamenode': '/home/cernypro/dev/source/ml4logs/data/interim/HDFS2/no_timestamps_cleaned/hadoop-hdfs-secondarynamenode-mesos-01/chunked_size_10_tokens_text',\n",
    "    'HDFS2-namenode': '/home/cernypro/dev/source/ml4logs/data/interim/HDFS2/no_timestamps_cleaned/hadoop-hdfs-namenode-mesos-01/chunked_size_10_tokens_text',\n",
    "    'HDFS2-datanode-01': '/home/cernypro/dev/source/ml4logs/data/interim/HDFS2/no_timestamps_cleaned/hadoop-hdfs-datanode-mesos-01/chunked_size_10_tokens_text',\n",
    "    'HDFS2-datanode-13': '/home/cernypro/dev/source/ml4logs/data/interim/HDFS2/no_timestamps_cleaned/hadoop-hdfs-datanode-mesos-13/chunked_size_10_tokens_text',\n",
    "    'Spark': '/home/cernypro/dev/source/ml4logs/data/interim/Spark/no_timestamps_spark/chunked_size_10_tokens_text',\n",
    "    'Zookeeper': '/home/cernypro/dev/source/ml4logs/data/interim/Zookeeper/no_timestamps_zookeeper/chunked_size_10_tokens_text',\n",
    "    'BGL': '/home/cernypro/dev/source/ml4logs/data/interim/BGL/no_timestamps_bgl/chunked_size_10_tokens_text',\n",
    "    'Hadoop': '/home/cernypro/dev/source/ml4logs/data/interim/Hadoop/no_timestamps_hadoop/chunked_size_10_tokens_text',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sized-certificate",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {name: load_from_disk(path) for name, path in used_dataset_paths.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "spare-anime",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train-data-HDFS1-cv1-1-time-ordered': Dataset({\n",
       "     features: ['chunk_text', 'chunk_tokens'],\n",
       "     num_rows: 905102\n",
       " }),\n",
       " 'HDFS2-secondarynamenode': Dataset({\n",
       "     features: ['chunk_text', 'chunk_tokens'],\n",
       "     num_rows: 71048\n",
       " }),\n",
       " 'HDFS2-namenode': Dataset({\n",
       "     features: ['chunk_text', 'chunk_tokens'],\n",
       "     num_rows: 1689280\n",
       " }),\n",
       " 'HDFS2-datanode-01': Dataset({\n",
       "     features: ['chunk_text', 'chunk_tokens'],\n",
       "     num_rows: 261472\n",
       " }),\n",
       " 'HDFS2-datanode-13': Dataset({\n",
       "     features: ['chunk_text', 'chunk_tokens'],\n",
       "     num_rows: 196808\n",
       " }),\n",
       " 'Spark': Dataset({\n",
       "     features: ['chunk_text', 'chunk_tokens'],\n",
       "     num_rows: 3323648\n",
       " }),\n",
       " 'Zookeeper': Dataset({\n",
       "     features: ['chunk_text', 'chunk_tokens'],\n",
       "     num_rows: 7432\n",
       " }),\n",
       " 'BGL': Dataset({\n",
       "     features: ['chunk_text', 'chunk_tokens'],\n",
       "     num_rows: 474792\n",
       " }),\n",
       " 'Hadoop': Dataset({\n",
       "     features: ['chunk_text', 'chunk_tokens'],\n",
       "     num_rows: 39336\n",
       " })}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "therapeutic-chamber",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train-data-HDFS1-cv1-1-time-ordered': 905102,\n",
       " 'HDFS2-secondarynamenode': 71048,\n",
       " 'HDFS2-namenode': 1689280,\n",
       " 'HDFS2-datanode-01': 261472,\n",
       " 'HDFS2-datanode-13': 196808,\n",
       " 'Spark': 3323648,\n",
       " 'Zookeeper': 7432,\n",
       " 'BGL': 474792,\n",
       " 'Hadoop': 39336}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_lengths = {name: len(ds) for name, ds in datasets.items()}\n",
    "ds_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "average-graphic",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_contexts_to_take = {\n",
    "    'train-data-HDFS1-cv1-1-time-ordered': 60000,\n",
    "    'HDFS2-secondarynamenode': 60000,\n",
    "    'HDFS2-namenode': 60000,\n",
    "    'HDFS2-datanode-01': 60000,\n",
    "    'HDFS2-datanode-13': 60000,\n",
    "    'Spark': 240000,\n",
    "    'Zookeeper': 60000,\n",
    "    'BGL': 60000,\n",
    "    'Hadoop': 60000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "looking-bearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_contexts_to_take = {name: min(ds_lengths[name], desired_contexts_to_take[name]) for name in desired_contexts_to_take}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "trained-conflict",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ceil(5.2).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "incredible-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_test_train_helper(dataset, name, desired_total_taken, val_ratio, rnd: np.random.Generator):\n",
    "    total_to_take = min(len(dataset), desired_total_taken)\n",
    "    val_size = int(np.ceil(total_to_take*val_ratio))\n",
    "    train_size = total_to_take - val_size\n",
    "    assert train_size + val_size == total_to_take\n",
    "    print(f'{name} - Train: {train_size}, val: {val_size}')\n",
    "    return dataset.train_test_split(test_size=val_size, train_size=train_size, generator=rnd, writer_batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "pending-reasoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 43\n",
    "rnd = np.random.default_rng(seed=SEED)\n",
    "VAL_RATIO = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "perfect-serbia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-data-HDFS1-cv1-1-time-ordered - Train: 57000, val: 3000\n",
      "HDFS2-secondarynamenode - Train: 57000, val: 3000\n",
      "HDFS2-namenode - Train: 57000, val: 3000\n",
      "HDFS2-datanode-01 - Train: 57000, val: 3000\n",
      "HDFS2-datanode-13 - Train: 57000, val: 3000\n",
      "Spark - Train: 228000, val: 12000\n",
      "Zookeeper - Train: 7060, val: 372\n",
      "BGL - Train: 57000, val: 3000\n",
      "Hadoop - Train: 37369, val: 1967\n"
     ]
    }
   ],
   "source": [
    "train_val_splits_datasets = {name: split_test_train_helper(dataset, name, desired_contexts_to_take[name], VAL_RATIO, rnd) for name, dataset in datasets.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "satisfactory-northern",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae253b84172749f4a6badb4e9e31b7b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=57.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa5c767a6cf4205853258e6440695d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=57.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36371f7451bd49cfa31f4380af1cd45b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=57.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-68b051791541>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_val_splits_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_val_splits_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-68b051791541>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_val_splits_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_val_splits_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/envs/huggingface_gpu/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m         }\n\u001b[1;32m    202\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/envs/huggingface_gpu/lib/python3.7/site-packages/datasets/fingerprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;31m# Call actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;31m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/envs/huggingface_gpu/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mflatten_indices\u001b[0;34m(self, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, new_fingerprint)\u001b[0m\n\u001b[1;32m   1854\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m             \u001b[0mdisable_nullable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable_nullable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1856\u001b[0;31m             \u001b[0mnew_fingerprint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_fingerprint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1857\u001b[0m         )\n\u001b[1;32m   1858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/envs/huggingface_gpu/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint)\u001b[0m\n\u001b[1;32m   1425\u001b[0m                 \u001b[0mfn_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m                 \u001b[0mnew_fingerprint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_fingerprint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m                 \u001b[0mupdate_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m             )\n\u001b[1;32m   1429\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/envs/huggingface_gpu/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m         }\n\u001b[1;32m    202\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/envs/huggingface_gpu/lib/python3.7/site-packages/datasets/fingerprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;31m# Call actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;31m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/envs/huggingface_gpu/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, update_data)\u001b[0m\n\u001b[1;32m   1690\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mdrop_last_batch\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m                             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m                         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m                         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Something simpler?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/envs/huggingface_gpu/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0mformat_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m             \u001b[0moutput_all_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_all_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1248\u001b[0;31m             \u001b[0mformat_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1249\u001b[0m         )\n\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/envs/huggingface_gpu/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[0;34m(self, key, format_type, format_columns, output_all_columns, format_kwargs)\u001b[0m\n\u001b[1;32m   1230\u001b[0m         \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m         pa_subtable = query_table(\n\u001b[0;32m-> 1232\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1233\u001b[0m         )\n\u001b[1;32m   1234\u001b[0m         formatted_output = format_table(\n",
      "\u001b[0;32m~/dev/envs/huggingface_gpu/lib/python3.7/site-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mquery_table\u001b[0;34m(pa_table, key, indices)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mpa_subtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_query_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mpa_subtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_query_table_with_indices_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpa_subtable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/envs/huggingface_gpu/lib/python3.7/site-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36m_query_table_with_indices_mapping\u001b[0;34m(pa_table, key, indices)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_range_contiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_query_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mpass\u001b[0m  \u001b[0;31m# treat as an iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/envs/huggingface_gpu/lib/python3.7/site-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36m_query_table\u001b[0;34m(pa_table, key)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# don't use pyarrow.Table.take even for pyarrow >=1.0 (see https://issues.apache.org/jira/browse/ARROW-9773)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_tables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0m_raise_bad_key_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/envs/huggingface_gpu/lib/python3.7/site-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.concat_tables\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/dev/envs/huggingface_gpu/lib/python3.7/site-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# don't use pyarrow.Table.take even for pyarrow >=1.0 (see https://issues.apache.org/jira/browse/ARROW-9773)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_tables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0m_raise_bad_key_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_datasets = {name: split['train'].flatten_indices() for name, split in train_val_splits_datasets.items()}\n",
    "val_datasets = {name: split['test'].flatten_indices() for name, split in train_val_splits_datasets.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "specified-jason",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(desired_contexts_to_take.keys()) == set(used_dataset_paths.keys()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ruled-reply",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = load_from_disk('/home/cernypro/dev/source/ml4logs/data/interim/HDFS2/no_timestamps_cleaned/hadoop-hdfs-secondarynamenode-mesos-01/chunked_size_10_tokens_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "black-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = rnd.choice(len(tst), 100, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "enormous-nowhere",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25916, 19006, 39723, 21737, 31075,  3040, 39801, 21281,  7718,\n",
       "       30934, 60144, 31883, 67125, 27506, 16985, 70681, 63906, 26383,\n",
       "       12290, 30328])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices[80:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
