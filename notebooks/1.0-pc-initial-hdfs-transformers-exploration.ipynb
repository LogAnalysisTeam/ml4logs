{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-3571b23bb2152210\n",
      "Reusing dataset text (/home/cernypro/.cache/huggingface/datasets/text/default-3571b23bb2152210/0.0.0/44d63bd03e7e554f16131765a251f2d8333a5fe8a73f6ea3de012dbc49443691)\n"
     ]
    }
   ],
   "source": [
    "hdfs1_dataset = load_dataset('text', data_files='../data/raw/HDFS1/HDFS.log', split='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '081109 203518 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.19.102:54106 dest: /10.250.19.102:50010'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs1_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/cernypro/.cache/huggingface/datasets/text/default-3571b23bb2152210/0.0.0/44d63bd03e7e554f16131765a251f2d8333a5fe8a73f6ea3de012dbc49443691/cache-15d3de4e024d5591.arrow\n"
     ]
    }
   ],
   "source": [
    "def remove_timestamp(example):\n",
    "    # need to find third occurence of a space and slice the string after it\n",
    "    # using a very non robust silly solution\n",
    "    s = example['text']\n",
    "    example['text'] = s[s.find(' ', s.find(' ', s.find(' ')+1)+1)+1:]\n",
    "    return example\n",
    "\n",
    "cleaned_dataset = hdfs1_dataset.map(remove_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_model_name = \"distilbert-base-cased\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_dataset(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\",\n",
    "                     truncation=True, max_length=256, return_special_tokens_mask=True)\n",
    "# tokenized_dataset = cleaned_dataset.map(tokenize_dataset, batched=True, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/cernypro/.cache/huggingface/datasets/text/default-3571b23bb2152210/0.0.0/44d63bd03e7e554f16131765a251f2d8333a5fe8a73f6ea3de012dbc49443691/cache-b9facbd72d2d63dd.arrow\n"
     ]
    }
   ],
   "source": [
    "def tokenize_dontpad_dataset(examples, tokenizer):\n",
    "    return tokenizer(examples['text'], truncation=True, return_special_tokens_mask=True)\n",
    "tokenized_unpadded_dataset = cleaned_dataset.map(tokenize_dontpad_dataset, fn_kwargs={'tokenizer': tokenizer}, batched=True, batch_size=1000, remove_columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '081109 203518 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.19.102:54106 dest: /10.250.19.102:50010'}\n",
      "{'text': 'INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.19.102:54106 dest: /10.250.19.102:50010'}\n",
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [101, 15969, 2271, 2346, 173, 22816, 119, 7154, 2249, 13040, 109, 7154, 3190, 23566, 1197, 131, 11336, 24271, 3510, 171, 10493, 168, 118, 7690, 1604, 1580, 1580, 1580, 1545, 1604, 1559, 1580, 16382, 22392, 26752, 1568, 1545, 188, 19878, 131, 120, 1275, 119, 4805, 119, 1627, 119, 9081, 131, 4335, 10424, 1545, 3532, 1204, 131, 120, 1275, 119, 4805, 119, 1627, 119, 9081, 131, 2260, 10424, 102], 'special_tokens_mask': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(hdfs1_dataset[idx])\n",
    "print(cleaned_dataset[idx])\n",
    "print(tokenized_unpadded_dataset[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "small_dataset = tokenized_unpadded_dataset.select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_dataset = tokenized_unpadded_dataset.train_test_split(train_size=100000, test_size=30000, shuffle=True, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'special_tokens_mask'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'special_tokens_mask'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _remove_unused_columns(model, dataset: \"datasets.Dataset\"):\n",
    "        # Inspect model forward signature to keep only the arguments it accepts.\n",
    "        signature = inspect.signature(model)\n",
    "        signature_columns = list(signature.parameters.keys())\n",
    "        # Labels may be named label or label_ids, the default data collator handles that.\n",
    "        signature_columns += [\"label\", \"label_ids\"]\n",
    "        columns = [k for k in signature_columns if k in dataset.column_names]\n",
    "        dataset.set_format(type=dataset.format[\"type\"], columns=columns)\n",
    "\n",
    "def eval_loop(model, dataloader: DataLoader, device):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    model.eval()\n",
    "    for idx, inputs in enumerate(dataloader):\n",
    "        t = torch.cuda.get_device_properties(0).total_memory\n",
    "        c = torch.cuda.memory_reserved(0)\n",
    "        a = torch.cuda.memory_allocated(0)\n",
    "        f = c-a  # free inside cache\n",
    "        print(f't={t} c={c} a={a} f={f}')\n",
    "        for k, v in inputs.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(device)\n",
    "        masked_idxs = inputs['labels'] != -100\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        prediction_correct = torch.argmax(outputs[1], dim=2)[masked_idxs] == inputs['labels'][masked_idxs]\n",
    "        total += prediction_correct.shape[0]\n",
    "        correct += torch.sum(prediction_correct).item()\n",
    "    return correct/total, correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = DistilBertForMaskedLM.from_pretrained(pretrained_model_name)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=34089730048 c=13331595264 a=1060333568 f=12271261696\n",
      "t=34089730048 c=16481517568 a=2634020352 f=13847497216\n",
      "t=34089730048 c=16481517568 a=2322252800 f=14159264768\n",
      "t=34089730048 c=16481517568 a=2337098752 f=14144418816\n",
      "t=34089730048 c=16481517568 a=2352192512 f=14129325056\n",
      "t=34089730048 c=16481517568 a=2366872576 f=14114644992\n",
      "t=34089730048 c=16481517568 a=2352192512 f=14129325056\n",
      "t=34089730048 c=16481517568 a=2084715520 f=14396802048\n",
      "(0.8088865656037637, 7737, 9565)\n"
     ]
    }
   ],
   "source": [
    "# _remove_unused_columns(model, small_dataset)\n",
    "torch.manual_seed(SEED)\n",
    "dataloader = DataLoader(small_dataset, batch_size=128, collate_fn=data_collator)\n",
    "print(eval_loop(model, dataloader, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "674"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del dataloader\n",
    "del model\n",
    "del trainer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mprokopcerny\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.20 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.19<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">Second-Training</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/prokopcerny/huggingface\" target=\"_blank\">https://wandb.ai/prokopcerny/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/prokopcerny/huggingface/runs/t7hvlkwy\" target=\"_blank\">https://wandb.ai/prokopcerny/huggingface/runs/t7hvlkwy</a><br/>\n",
       "                Run data is saved locally in <code>/home/cernypro/dev/source/huggingface_gpu/notebooks/wandb/run-20210224_143834-t7hvlkwy</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='3910' max='3910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3910/3910 35:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.980700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.975500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.816600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.781900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.770800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.752900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.735300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.705200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.698300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.691900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.681800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.678600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.672800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.668700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.672900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.662300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.654200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.654100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.646800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.654600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.649800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.650700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.649300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.645300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.648600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.633600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.645400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.645900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.641700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.641400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.635400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.640400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.635200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.635000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.634000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.638500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.635000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.628800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.630000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(output_dir=\"./second_train_experiment\",\n",
    "                                  num_train_epochs=5,\n",
    "                                  per_device_eval_batch_size=256, \n",
    "                                  per_device_train_batch_size=128,\n",
    "                                  warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "                                  weight_decay=0.01,               # strength of weight decay\n",
    "                                  logging_dir='./logs',            # directory for storing logs\n",
    "                                  logging_steps=100,\n",
    "                                  save_steps=1000,\n",
    "                                  run_name=\"Second-Training\")\n",
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  data_collator=data_collator,\n",
    "                  train_dataset=train_test_dataset['train'],\n",
    "                  eval_dataset=train_test_dataset['test']\n",
    "                  )\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=34089730048 c=622854144 a=527545344 f=95308800\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "c = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = c-a  # free inside cache\n",
    "print(f't={t} c={c} a={a} f={f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_size(size):\n",
    "    \"\"\"Pretty prints a torch.Size object\"\"\"\n",
    "    assert(isinstance(size, torch.Size))\n",
    "    return \" × \".join(map(str, size))\n",
    "\n",
    "def dump_tensors(gpu_only=True):\n",
    "\t\"\"\"Prints a list of the Tensors being tracked by the garbage collector.\"\"\"\n",
    "\timport gc\n",
    "\ttotal_size = 0\n",
    "\tfor obj in gc.get_objects():\n",
    "\t\ttry:\n",
    "\t\t\tif torch.is_tensor(obj):\n",
    "\t\t\t\tif not gpu_only or obj.is_cuda:\n",
    "\t\t\t\t\tprint(\"%s:%s%s %s\" % (type(obj).__name__, \n",
    "\t\t\t\t\t\t\t\t\t\t  \" GPU\" if obj.is_cuda else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t  \" pinned\" if obj.is_pinned else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t  pretty_size(obj.size())))\n",
    "\t\t\t\t\ttotal_size += obj.numel()\n",
    "\t\t\telif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\n",
    "\t\t\t\tif not gpu_only or obj.is_cuda:\n",
    "\t\t\t\t\tprint(\"%s → %s:%s%s%s%s %s\" % (type(obj).__name__, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   type(obj.data).__name__, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" GPU\" if obj.is_cuda else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" pinned\" if obj.data.is_pinned else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" grad\" if obj.requires_grad else \"\", \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" volatile\" if obj.volatile else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   pretty_size(obj.data.size())))\n",
    "\t\t\t\t\ttotal_size += obj.data.numel()\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tpass        \n",
    "\tprint(\"Total size:\", total_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/appl/software/PyTorch/1.7.1-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:126: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: GPU pinned 28996 × 768\n",
      "Parameter: GPU pinned 512 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 28996\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 3072 × 768\n",
      "Parameter: GPU pinned 3072\n",
      "Parameter: GPU pinned 768 × 3072\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 28996 × 768\n",
      "Parameter: GPU pinned 512 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 28996\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 3072 × 768\n",
      "Parameter: GPU pinned 3072\n",
      "Parameter: GPU pinned 768 × 3072\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 3072 × 768\n",
      "Parameter: GPU pinned 3072\n",
      "Parameter: GPU pinned 768 × 3072\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 3072 × 768\n",
      "Parameter: GPU pinned 3072\n",
      "Parameter: GPU pinned 768 × 3072\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 3072 × 768\n",
      "Parameter: GPU pinned 3072\n",
      "Parameter: GPU pinned 768 × 3072\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 3072 × 768\n",
      "Parameter: GPU pinned 3072\n",
      "Parameter: GPU pinned 768 × 3072\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 3072 × 768\n",
      "Parameter: GPU pinned 3072\n",
      "Parameter: GPU pinned 768 × 3072\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 3072 × 768\n",
      "Parameter: GPU pinned 3072\n",
      "Parameter: GPU pinned 768 × 3072\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 3072 × 768\n",
      "Parameter: GPU pinned 3072\n",
      "Parameter: GPU pinned 768 × 3072\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 3072 × 768\n",
      "Parameter: GPU pinned 3072\n",
      "Parameter: GPU pinned 768 × 3072\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 3072 × 768\n",
      "Parameter: GPU pinned 3072\n",
      "Parameter: GPU pinned 768 × 3072\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 3072 × 768\n",
      "Parameter: GPU pinned 3072\n",
      "Parameter: GPU pinned 768 × 3072\n",
      "Parameter: GPU pinned 768\n",
      "Total size: 131624072\n"
     ]
    }
   ],
   "source": [
    "dump_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}