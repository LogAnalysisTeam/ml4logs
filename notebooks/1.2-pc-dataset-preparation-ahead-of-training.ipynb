{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "advance-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from pathlib import Path\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from typing import List\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "collective-soundtrack",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_base_dir = Path.cwd().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "organizational-basket",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = \"distilbert-base-cased\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "unsigned-brisbane",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-f7d20bad4b8d075b\n",
      "Reusing dataset text (/home/cernypro/.cache/huggingface/datasets/text/default-f7d20bad4b8d075b/0.0.0/44d63bd03e7e554f16131765a251f2d8333a5fe8a73f6ea3de012dbc49443691)\n"
     ]
    }
   ],
   "source": [
    "hdfs1_path = project_base_dir / 'data' / 'raw' / 'HDFS1' / 'HDFS.log'\n",
    "hdfs1_dataset = load_dataset('text', data_files=str(hdfs1_path), split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "mathematical-freeze",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/cernypro/.cache/huggingface/datasets/text/default-f7d20bad4b8d075b/0.0.0/44d63bd03e7e554f16131765a251f2d8333a5fe8a73f6ea3de012dbc49443691/cache-3f531662155381a4.arrow\n"
     ]
    }
   ],
   "source": [
    "def remove_timestamp(example):\n",
    "    # need to find third occurence of a space and slice the string after it\n",
    "    # using a very non robust silly solution\n",
    "    s = example['text']\n",
    "    example['text'] = s[s.find(' ', s.find(' ', s.find(' ')+1)+1)+1:]\n",
    "    return example\n",
    "\n",
    "cleaned_dataset = hdfs1_dataset.map(remove_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baking-grill",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_hdfs1_path = project_base_dir / 'data' / 'interim' / 'HDFS1_no_timestamp'\n",
    "cleaned_dataset.save_to_disk(cleaned_hdfs1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "spare-collection",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/cernypro/.cache/huggingface/datasets/text/default-f7d20bad4b8d075b/0.0.0/44d63bd03e7e554f16131765a251f2d8333a5fe8a73f6ea3de012dbc49443691/cache-c9d842d71543c5b3.arrow\n"
     ]
    }
   ],
   "source": [
    "def tokenize_no_special_tokens(examples, tokenizer):\n",
    "    return {'tokens': tokenizer(examples['text'], add_special_tokens=False, truncation=True, return_attention_mask=False)['input_ids']}\n",
    "purely_tokenized = cleaned_dataset.map(tokenize_no_special_tokens, fn_kwargs={'tokenizer': tokenizer}, batched=True, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "rapid-judgment",
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_tokenized_hdfs1_path = project_base_dir / 'data' / 'interim' / 'HDFS1_tokenized_no_special_tokens'\n",
    "purely_tokenized.save_to_disk(pure_tokenized_hdfs1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "emerging-garlic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def chunkify(examples):\n",
    "    return {\"chunk\": [examples['tokens']]}\n",
    "chunked_size_10 = purely_tokenized.map(chunkify,\n",
    "                                       batched=True,\n",
    "                                       batch_size=10,\n",
    "                                       drop_last_batch=True,\n",
    "                                       remove_columns=purely_tokenized.column_names,\n",
    "                                       num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "medieval-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_10_hdfs1_path = project_base_dir / 'data' / 'interim' / 'HDFS1_tokenized_chunked_size_10'\n",
    "chunked_size_10.save_to_disk(chunked_10_hdfs1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tropical-spirituality",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_size_10 = load_from_disk(str(project_base_dir / 'data' / 'interim' / 'HDFS1_tokenized_chunked_size_10'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "southern-heater",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/cernypro/dev/source/ml4logs/data/processed')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path('/home/cernypro/dev/source/ml4logs/data/processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "domestic-ranch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chunk'],\n",
       "    num_rows: 1117560\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_size_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "miniature-calcium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_and_flat_context(context: List[List[int]], rnd: np.random.Generator, remove_target_prob:float):\n",
    "    target_idx = rnd.integers(low=0, high=len(context))\n",
    "    remove_target = rnd.random() < remove_target_prob\n",
    "    target_sentence = context[target_idx]\n",
    "    processed_context = context[:target_idx] + context[target_idx + remove_target:]\n",
    "    flattened_context = [token for sentence in context for token in sentence]\n",
    "    return target_sentence, flattened_context\n",
    "\n",
    "def prepare_ict(examples, epochs, rnd: np.random.Generator, remove_target_prob:float):\n",
    "    targets = []\n",
    "    flat_contexts = []\n",
    "    for context in examples['chunk']:\n",
    "        for _ in range(epochs):\n",
    "            t, f = create_target_and_flat_context(context, rnd, remove_target_prob)\n",
    "            targets.append(t)\n",
    "            flat_contexts.append(f)\n",
    "    return {'target': targets,\n",
    "            'flat_context': flat_contexts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "frozen-flush",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1afc256912744e5ab7b17a3e915ebe92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rnd = np.random.default_rng(0)\n",
    "one = chunked_size_10.select(range(10000)).map(prepare_ict, fn_kwargs={'epochs':4, 'rnd':rnd, 'remove_target_prob':0.9}, batched=True, batch_size=500, remove_columns=chunked_size_10.column_names, load_from_cache_file=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "creative-northeast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['flat_context', 'target'],\n",
       "    num_rows: 40000\n",
       "})"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "stock-saskatchewan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['flat_context', 'target'],\n",
       "    num_rows: 60000\n",
       "})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-tenant",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
