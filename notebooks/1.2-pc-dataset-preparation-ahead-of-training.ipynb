{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "synthetic-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "from transformers import DistilBertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "coated-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_base_dir = Path.cwd().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bottom-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = \"distilbert-base-cased\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "lined-version",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-f7d20bad4b8d075b\n",
      "Reusing dataset text (/home/cernypro/.cache/huggingface/datasets/text/default-f7d20bad4b8d075b/0.0.0/44d63bd03e7e554f16131765a251f2d8333a5fe8a73f6ea3de012dbc49443691)\n"
     ]
    }
   ],
   "source": [
    "hdfs1_path = project_base_dir / 'data' / 'raw' / 'HDFS1' / 'HDFS.log'\n",
    "hdfs1_dataset = load_dataset('text', data_files=str(hdfs1_path), split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "clinical-hands",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/cernypro/.cache/huggingface/datasets/text/default-f7d20bad4b8d075b/0.0.0/44d63bd03e7e554f16131765a251f2d8333a5fe8a73f6ea3de012dbc49443691/cache-3f531662155381a4.arrow\n"
     ]
    }
   ],
   "source": [
    "def remove_timestamp(example):\n",
    "    # need to find third occurence of a space and slice the string after it\n",
    "    # using a very non robust silly solution\n",
    "    s = example['text']\n",
    "    example['text'] = s[s.find(' ', s.find(' ', s.find(' ')+1)+1)+1:]\n",
    "    return example\n",
    "\n",
    "cleaned_dataset = hdfs1_dataset.map(remove_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "changed-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_hdfs1_path = project_base_dir / 'data' / 'interim' / 'HDFS1_no_timestamp'\n",
    "cleaned_dataset.save_to_disk(cleaned_hdfs1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "innocent-minister",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/cernypro/.cache/huggingface/datasets/text/default-f7d20bad4b8d075b/0.0.0/44d63bd03e7e554f16131765a251f2d8333a5fe8a73f6ea3de012dbc49443691/cache-c9d842d71543c5b3.arrow\n"
     ]
    }
   ],
   "source": [
    "def tokenize_no_special_tokens(examples, tokenizer):\n",
    "    return {'tokens': tokenizer(examples['text'], add_special_tokens=False, truncation=True, return_attention_mask=False)['input_ids']}\n",
    "purely_tokenized = cleaned_dataset.map(tokenize_no_special_tokens, fn_kwargs={'tokenizer': tokenizer}, batched=True, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "hairy-baghdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_tokenized_hdfs1_path = project_base_dir / 'data' / 'interim' / 'HDFS1_tokenized_no_special_tokens'\n",
    "purely_tokenized.save_to_disk(pure_tokenized_hdfs1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "involved-bikini",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def chunkify(examples):\n",
    "    return {\"chunk\": [examples['tokens']]}\n",
    "chunked_size_10 = purely_tokenized.map(chunkify,\n",
    "                                       batched=True,\n",
    "                                       batch_size=10,\n",
    "                                       drop_last_batch=True,\n",
    "                                       remove_columns=purely_tokenized.column_names,\n",
    "                                       num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "attempted-torture",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_10_hdfs1_path = project_base_dir / 'data' / 'interim' / 'HDFS1_tokenized_chunked_size_10'\n",
    "chunked_size_10.save_to_disk(chunked_10_hdfs1_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
