{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fifteen-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from typing import List, Union, Dict\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoModel, TrainingArguments, Trainer\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ambient-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_mask(padded_batch, pad_token=0):\n",
    "    return (padded_batch != pad_token).to(torch.uint8)\n",
    "\n",
    "\n",
    "def _pad_truncate_add_special_tokens(batch: List[List[int]], max_len, pad_token=0, start_token=101, sep_token=102):\n",
    "    sequence_lengths = torch.tensor([min(max_len-2, len(seq)) for seq in batch], dtype=torch.int64)\n",
    "    batch_max_len = sequence_lengths.max()\n",
    "    padded_batch = torch.full(size=(len(batch), batch_max_len+2), fill_value=pad_token, dtype=torch.int64)\n",
    "    padded_batch[:, 0] = start_token\n",
    "    for seq_idx, seq in enumerate(batch):\n",
    "        padded_batch[seq_idx, 1:sequence_lengths[seq_idx]+1] = torch.tensor(seq[:sequence_lengths[seq_idx]], dtype=torch.int64)\n",
    "        padded_batch[seq_idx, sequence_lengths[seq_idx]+1] = sep_token\n",
    "    mask = _make_mask(padded_batch)\n",
    "    return padded_batch, mask\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForPreprocessedICT:\n",
    "    target_max_seq:int = 512\n",
    "    context_max_seq:int = 512\n",
    "    start_token:int = 101 # [CLS]\n",
    "    sep_token:int = 102 # [SEP]\n",
    "    pad_token:int = 0\n",
    "        \n",
    "    def _pad_truncate_add_special_tokens(self, batch: List[List[int]], max_len):\n",
    "        return _pad_truncate_add_special_tokens(batch, max_len=max_len, pad_token=self.pad_token, start_token=self.start_token, sep_token=self.sep_token)\n",
    "             \n",
    "    def __call__(self, contexts: List[Dict[str, List[int]]]):\n",
    "        if isinstance(contexts[0], dict):\n",
    "            target_sentences = [context_dict['target'] for context_dict in contexts]\n",
    "            flattened_contexts = [context_dict['flat_context'] for context_dict in contexts]\n",
    "        correct_class = torch.arange(len(target_sentences), dtype=torch.int64)\n",
    "        padded_target_batch, padded_target_mask = self._pad_truncate_add_special_tokens(target_sentences, self.target_max_seq)\n",
    "        padded_context_batch, padded_context_mask = self._pad_truncate_add_special_tokens(flattened_contexts, self.context_max_seq)\n",
    "        return {'target': padded_target_batch,\n",
    "                'target_mask': padded_target_mask,\n",
    "                'context': padded_context_batch,\n",
    "                'context_mask': padded_context_mask,\n",
    "                'correct_class': correct_class}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "vanilla-swiss",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForPreprocessedICT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "prerequisite-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = load_from_disk('/home/cernypro/dev/source/ml4logs/data/processed/Tr-1093568_Ev-6400_Epochs-3_Seed-13/eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "attached-headset",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = \"distilbert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "nearby-blood",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClsEncoderTower(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Simple model on top of a BERT like model.\n",
    "    It's a linear layer on the [CLS] token of each sentence from BERT.\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_model_name_or_path, output_encode_dimension=512):\n",
    "        super(ClsEncoderTower, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(pretrained_model_name_or_path)\n",
    "        self.linear = torch.nn.Linear(self.bert.config.dim, output_encode_dimension) # self.bert.config.dim most likely 768\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_token_embedding = bert_output[0][:, 0]\n",
    "        cls_encoding = self.linear(cls_token_embedding)\n",
    "        return cls_encoding\n",
    "    \n",
    "class OneTowerICT(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Network for the inverse close task, uses one BERT tower for creating encodings of target and context sentences (query and document as per nomenclature of original paper)\n",
    "    Uses cross entropy loss\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_model_name_or_path, output_encode_dimension=512):\n",
    "        super(OneTowerICT, self).__init__()\n",
    "        self.tower = ClsEncoderTower(pretrained_model_name_or_path, output_encode_dimension)\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    def forward(self, target, target_mask, context, context_mask, correct_class):\n",
    "        target_cls_encode = self.tower(input_ids=target, attention_mask=target_mask)\n",
    "        context_cls_encode = self.tower(input_ids=context, attention_mask=context_mask)\n",
    "        \n",
    "        logits = torch.matmul(target_cls_encode, context_cls_encode.transpose(-2, -1))\n",
    "        loss = self.loss_fn(logits, correct_class)\n",
    "        return loss, target_cls_encode, context_cls_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "peaceful-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_dir = Path.cwd().parent / 'models' / '1T_Eps_2_Lines_8000000_T-len_512_C-len_512_Tr-batch_64_Ev-b_64_O-dim_512'\n",
    "saved_model_file = saved_model_dir / 'pytorch_model.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "responsible-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(saved_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "liked-zimbabwe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneTowerICT(\n",
       "  (tower): ClsEncoderTower(\n",
       "    (bert): DistilBertModel(\n",
       "      (embeddings): Embeddings(\n",
       "        (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer): Transformer(\n",
       "        (layer): ModuleList(\n",
       "          (0): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=768, out_features=512, bias=True)\n",
       "  )\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = OneTowerICT(pretrained_model_name)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "israeli-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=f\"./tmpModelDir\",\n",
    "                                      per_device_eval_batch_size=64, \n",
    "                                      per_device_train_batch_size=64,\n",
    "                                      warmup_steps=10,                # number of warmup steps for learning rate scheduler\n",
    "                                      weight_decay=0.01,               # strength of weight decay\n",
    "                                      logging_dir='../logs',            # directory for storing logs\n",
    "                                      logging_steps=10,\n",
    "                                      logging_first_step=True,\n",
    "                                      eval_steps=20,\n",
    "                                      evaluation_strategy='steps',\n",
    "                                      prediction_loss_only=True,\n",
    "                                      save_steps=100,\n",
    "                                      save_total_limit=15,\n",
    "                                      label_names=['target', 'context'],\n",
    "                                      seed=42,\n",
    "                                      report_to=[],\n",
    "                                      remove_unused_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "respective-borough",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                      args=training_args,\n",
    "                      data_collator=data_collator,\n",
    "                      eval_dataset=eval_dataset\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "coupled-craft",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6826252937316895,\n",
       " 'eval_runtime': 35.2668,\n",
       " 'eval_samples_per_second': 181.474}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-expansion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
