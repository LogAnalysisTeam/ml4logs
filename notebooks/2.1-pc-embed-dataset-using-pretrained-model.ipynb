{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "scheduled-retreat",
   "metadata": {},
   "source": [
    "# Log line embedding using a BERT-based encoder\n",
    "\n",
    "This notebook showcases how to use a pretrained DistilBert based model to embed log lines from text into a vector space, using Huggingface Transformers and Datasets libraries.\n",
    "\n",
    "Note: This notebook assumes [Cookiecutter datascience](https://drivendata.github.io/cookiecutter-data-science/) directory structure of the project, and expects to be in /notebooks/ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "compatible-diving",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Union, Dict, Optional\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertPreTrainedModel, DistilBertModel\n",
    "from transformers.file_utils import ModelOutput\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-transportation",
   "metadata": {},
   "source": [
    "Setup general used objects and constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "powered-recovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_base_dir = Path.cwd().parent\n",
    "data_dir = project_base_dir / 'data'\n",
    "base_pretrained_model_name = \"distilbert-base-cased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-relevance",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "First we load HDFS1 dataset and select first 1000 lines from it as a demonstrative subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "comparative-morocco",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b1cdfca81a0e7c30\n",
      "Reusing dataset text (/home/cernypro/.cache/huggingface/datasets/text/default-b1cdfca81a0e7c30/0.0.0/44d63bd03e7e554f16131765a251f2d8333a5fe8a73f6ea3de012dbc49443691)\n"
     ]
    }
   ],
   "source": [
    "dataset_path = Path('/home/cernypro/dev/source/ml4logs/data/interim/HDFS1/train-data-HDFS1-cv1-1.log')\n",
    "dataset_name = dataset_path.stem\n",
    "dataset = load_dataset('text', data_files=str(dataset_path), split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-constitutional",
   "metadata": {},
   "source": [
    "Now we perform a rudimentary log-line preprocessing, removing the timestamp from each line (note, the model used in this notebook was pretrained with such preprocessing done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "innocent-holder",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/cernypro/.cache/huggingface/datasets/text/default-b1cdfca81a0e7c30/0.0.0/44d63bd03e7e554f16131765a251f2d8333a5fe8a73f6ea3de012dbc49443691/cache-c63d3274012f7e27.arrow\n"
     ]
    }
   ],
   "source": [
    "HDFS1_TIMESTAMP_PATTERN = re.compile(r'^(\\d+) (\\d+) (\\d+) ')\n",
    "def remove_timestamp(example):\n",
    "    example['text'] = HDFS1_TIMESTAMP_PATTERN.sub('', example['text'])\n",
    "    return example\n",
    "\n",
    "cleaned_dataset = dataset.select(range(1000)).map(remove_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-property",
   "metadata": {},
   "source": [
    "## Transformer model preparation\n",
    "Here we'll prepare the Transformer model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "greenhouse-humanity",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EmbeddingOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    ModelOutput class inspired per Huggingface Transformers library conventions, may be replaced by a suitable alternative class from the library if any exists.\n",
    "    \"\"\"\n",
    "    embedding: torch.FloatTensor = None\n",
    "        \n",
    "class DistilBertForClsEmbedding(DistilBertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    DistilBertModel with a linear layer applied to [CLS] token.\n",
    "    Initialize using .from_pretrained(path_or_model_name) method\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        if config.task_specific_params is None:\n",
    "            config.task_specific_params = dict()\n",
    "\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "        self.cls_projector = torch.nn.Linear(config.dim, config.task_specific_params.setdefault('cls_embedding_dimension', 512))\n",
    "\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_output = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_token_embedding = bert_output.last_hidden_state[:, 0]\n",
    "        cls_encoding = self.cls_projector(cls_token_embedding)\n",
    "        return EmbeddingOutput(embedding=cls_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-junior",
   "metadata": {},
   "source": [
    "Now load the model from checkpoint and prepare it's tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "after-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_directory = project_base_dir / 'models' / 'LogEncoder_from_1T_Eps_1_M_basic_chunked_10_Seed-42_T-len_512_C-len_512_Tr-batch_64_Ev-b_64_O-dim_100'\n",
    "\n",
    "encoder_model = DistilBertForClsEmbedding.from_pretrained(embedding_model_directory).to('cuda')\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(base_pretrained_model_name)  # The tokenizer must match the one used for the saved model, this model uses distilbert-base-cased tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "liked-hotel",
   "metadata": {},
   "outputs": [],
   "source": [
    "line = 'INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_200811101024_0001_m_000541_0/part-00541. blk_-3113070658935403731'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "biological-imperial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.3471140e-01, -9.3393378e-02, -5.8012033e-01, -8.4221125e-01,\n",
       "        -2.5787169e-01, -1.7984155e-01,  6.0841136e-02,  3.7366140e-01,\n",
       "         4.5528108e-01, -1.4168188e-01,  3.3963236e-01,  5.3392655e-01,\n",
       "         1.0305425e-01, -1.2355834e-01, -7.2257012e-01,  2.2690922e-01,\n",
       "         6.4670700e-01,  1.0595173e+00,  3.0549049e-01, -1.4099821e-02,\n",
       "        -2.2744636e-01, -1.9417098e-01, -3.3728071e-02, -5.8352685e-01,\n",
       "        -3.4068272e-01,  3.7235403e-01,  5.1490935e-03, -3.1225786e-01,\n",
       "         3.4704870e-01,  2.1292098e-01, -5.4918939e-01, -1.7601267e-02,\n",
       "         5.2804511e-02,  4.1435388e-01,  3.6859244e-01, -2.0148341e-01,\n",
       "        -1.9323207e-02,  6.5854901e-01,  1.0366077e-01, -4.8482671e-01,\n",
       "        -3.8870147e-01,  1.6247231e-01,  1.2758509e+00,  7.4110992e-02,\n",
       "         2.1559796e-01, -6.6705936e-01, -3.3096114e-01, -3.6413753e-01,\n",
       "         8.3985049e-01,  2.6944232e-01, -3.0135852e-01,  1.4911635e-01,\n",
       "         8.2357633e-01, -7.0754844e-01,  1.4882492e+00, -1.3171208e-01,\n",
       "         7.2327691e-01,  2.9527340e-03, -8.8290948e-01, -3.5225898e-01,\n",
       "        -4.2177454e-01,  7.1636945e-01, -1.6397849e-01,  2.1690761e-01,\n",
       "        -3.1021154e-01,  3.8424692e-01, -5.6924355e-01, -4.0523633e-01,\n",
       "        -4.5617753e-01, -3.4165588e-01,  1.4603013e-01,  1.0807350e-01,\n",
       "        -1.1564357e+00,  1.7152947e-01, -9.6279246e-01,  1.0808538e+00,\n",
       "        -9.1268621e-02,  4.7859859e-01,  4.1196069e-01,  3.5181877e-01,\n",
       "         1.9488561e-01, -2.2583655e-01, -1.1021366e-01,  2.3783886e-01,\n",
       "        -4.4042584e-01,  3.8267300e-04,  4.1395754e-01,  8.4316991e-02,\n",
       "        -1.0589652e-01, -8.9930499e-01,  6.8193316e-02,  6.2153018e-01,\n",
       "        -3.6233610e-01,  9.3367910e-01,  7.8300726e-01, -4.6547043e-01,\n",
       "        -4.0823942e-01, -2.6832533e-01, -2.6719600e-01, -1.7199340e-01]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_model(**tokenizer([line],\n",
    "                                        return_tensors='pt',\n",
    "                                        truncation=True,\n",
    "                                        padding=True).to('cuda')\n",
    "                            ).embedding.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-brave",
   "metadata": {},
   "source": [
    "Here we'll prepare the encode function which we will map over our dataset, which will add an embedding column to our data containing the vector embeddings for each log-line.\n",
    "\n",
    "We will then apply this function in batches (for faster processing) as both our tokenizer and model can handle data in batched form. The batch size was chosen arbitrarily.\n",
    "\n",
    "Our encode function takes two additional arguments which have to be passed as a dict fn_kwargs to the map function. (We could also use closures, but I find this cleaner and easier to copy into a script from a notebook environment)\n",
    "\n",
    "See [Datasets .map documentation](https://huggingface.co/docs/datasets/processing.html#processing-data-with-map) for more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "legislative-thousand",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2609966285844c23924944f9acd4907c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=391.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def encode(examples, tokenizer, encoder):\n",
    "    with torch.no_grad():\n",
    "        embedding = encoder(**tokenizer(examples['text'],\n",
    "                                        return_tensors='pt',\n",
    "                                        truncation=True,\n",
    "                                        padding=True).to('cuda')\n",
    "                            ).embedding.cpu().detach().numpy()\n",
    "    return {'embedding': embedding}\n",
    "\n",
    "encoder_model.eval()\n",
    "embedded_dataset = cleaned_dataset.map(encode,\n",
    "                                       fn_kwargs={'tokenizer': tokenizer,\n",
    "                                                  'encoder': encoder_model},\n",
    "                                       batched=True,\n",
    "                                       batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-dover",
   "metadata": {},
   "source": [
    "Here we can see first three embeddings alongside their original lines using the slicing notation, which Datasets supports. \n",
    "The returned object is a dict with column names as keys and lists of the column contents as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "trying-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = data_dir / 'processed' / dataset_name / f\"embedding_from_{embedding_model_directory.stem}\"\n",
    "embedded_dataset.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "figured-potential",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embedding': Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None),\n",
       " 'text': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "expressed-differential",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a a'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer(\"a a\", add_special_tokens=False, truncation=True, return_attention_mask=False)['input_ids'], clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "failing-bobby",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INFO dfs.FSDataset: Deleting block blk_-7013325917247206057 file /mnt/hadoop/dfs/data/current/subdir10/blk_-7013325917247206057'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_dataset[-1]['text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
