{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dental-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/cernypro/dev/source/ml4logs/src/bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "naked-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoders import DistilBertForClsEmbedding\n",
    "from ict import OneTowerICT, TwoTowerICT\n",
    "from dataset_utils import my_caching_load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rocky-scholarship",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Union, Dict, Optional\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, AutoModel, DistilBertPreTrainedModel, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "organizational-blackjack",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-35c85ea2505ebc64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /data/temporary/huggingface_2406238/datasets/text/default-35c85ea2505ebc64/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /data/temporary/huggingface_2406238/datasets/text/default-35c85ea2505ebc64/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "hdfs1_dataset = load_dataset('text', data_files='/home/cernypro/dev/source/ml4logs/data/interim/HDFS1/no_timestamps_test-data-HDFS1.log', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "accredited-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_raw_dataset = hdfs1_dataset.select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ignored-parking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49536b38f97744b9b5637a9912671c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_timestamp(example):\n",
    "    # need to find third occurence of a space and slice the string after it\n",
    "    # using a very non robust silly solution\n",
    "    #s = example['text']\n",
    "    #example['text'] = s[s.find(' ', s.find(' ', s.find(' ')+1)+1)+1:]\n",
    "    return example\n",
    "\n",
    "small_cleaned_dataset = small_raw_dataset.map(remove_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "working-coaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = \"distilbert-base-cased\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "interim-surname",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClsEncoderTower(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Simple model on top of a BERT like model.\n",
    "    It's a linear layer on the [CLS] token of each sentence from BERT.\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_model_name_or_path, output_encode_dimension=512):\n",
    "        super(ClsEncoderTower, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(pretrained_model_name_or_path)\n",
    "        self.linear = torch.nn.Linear(self.bert.config.dim, output_encode_dimension) # self.bert.config.dim most likely 768\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_token_embedding = bert_output[0][:, 0]\n",
    "        cls_encoding = self.linear(cls_token_embedding)\n",
    "        return cls_encoding\n",
    "\n",
    "\n",
    "class OneTowerICT(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Network for the inverse close task, uses one BERT tower for creating encodings of target and context sentences (query and document as per nomenclature of original paper)\n",
    "    Uses cross entropy loss\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_model_name_or_path, output_encode_dimension=512):\n",
    "        super(OneTowerICT, self).__init__()\n",
    "        self.tower = ClsEncoderTower(pretrained_model_name_or_path, output_encode_dimension)\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    def forward(self, target, target_mask, context, context_mask, correct_class):\n",
    "        target_cls_encode = self.tower(input_ids=target, attention_mask=target_mask)\n",
    "        context_cls_encode = self.tower(input_ids=context, attention_mask=context_mask)\n",
    "        \n",
    "        logits = torch.matmul(target_cls_encode, context_cls_encode.transpose(-2, -1))\n",
    "        loss = self.loss_fn(logits, correct_class)\n",
    "        return loss, target_cls_encode, context_cls_encode\n",
    "    \n",
    "class TwoTowerICT(torch.nn.Module):\n",
    "    def __init__(self, target_tower_pretrained_model_name_or_path, context_tower_pretrained_model_name_or_path=None, output_encode_dimension=512):\n",
    "        super(TwoTowerICT, self).__init__()\n",
    "        assert target_tower_pretrained_model_name_or_path is not None, \"Target tower pretrained model must me specified!\"\n",
    "        if context_tower_pretrained_model_name_or_path is None:\n",
    "            context_tower_pretrained_model_name_or_path = target_tower_pretrained_model_name_or_path\n",
    "        self.target_encoder = ClsEncoderTower(target_tower_pretrained_model_name_or_path, output_encode_dimension)\n",
    "        self.context_encoder = ClsEncoderTower(context_tower_pretrained_model_name_or_path, output_encode_dimension)\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, target, target_mask, context, context_mask, correct_class):\n",
    "        target_cls_encode = self.target_encoder(input_ids=target, attention_mask=target_mask)\n",
    "        context_cls_encode = self.context_encoder(input_ids=context, attention_mask=context_mask)\n",
    "        \n",
    "        logits = torch.matmul(target_cls_encode, context_cls_encode.transpose(-2, -1))\n",
    "        loss = self.loss_fn(logits, correct_class)\n",
    "        return loss, target_cls_encode, context_cls_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "disabled-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_dir = Path('/home/cernypro/dev/source/ml4logs/models/ICT/2T_Eps_1_M_basic_chunked_10_Seed-42_T-len_512_C-len_512_Tr-batch_64_Ev-b_64_O-dim_100')\n",
    "saved_model_file = saved_model_dir / 'pytorch_model.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "offensive-narrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dict = torch.load(saved_model_file, map_location=torch.device('cpu'))\n",
    "state_dict = torch.load(saved_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "surprised-tunnel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TwoTowerICT(pretrained_model_name, output_encode_dimension=100)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "removed-finnish",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "encoder = model.context_encoder\n",
    "encoder = encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "champion-integration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8415d69e224623a5a8c0ac48357fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def encode(examples, tokenizer, encoder):\n",
    "    return {'embedding': encoder(**tokenizer(examples['text'], return_tensors='pt', truncation=True, padding=True).to(device)).cpu().detach().numpy().tolist()}\n",
    "\n",
    "small_embedded_dataset = small_cleaned_dataset.map(encode, fn_kwargs={'tokenizer': tokenizer, 'encoder': encoder}, batched=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-bleeding",
   "metadata": {},
   "source": [
    "# Transforming into a huggingface PretrainedModel\n",
    "An experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "registered-orange",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.file_utils import ModelOutput\n",
    "from transformers import DistilBertModel\n",
    "\n",
    "@dataclass\n",
    "class EmbeddingOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    ModelOutput class inspired per Huggingface Transformers library conventions, may be replaced by a suitable alternative class from the library if any exists.\n",
    "    \"\"\"\n",
    "    embedding: torch.FloatTensor = None\n",
    "        \n",
    "class DistilBertForClsEmbedding(DistilBertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    DistilBertModel with a linear layer applied to [CLS] token.\n",
    "    Initialize using .from_pretrained(path_or_model_name) method\n",
    "    use task_specific_params={'cls_embedding_dimension': *YOUR EMBEDDING DIMENSION HERE*} to set embedding dimension\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        if config.task_specific_params is None:\n",
    "            config.task_specific_params = dict()\n",
    "\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "        self.cls_projector = torch.nn.Linear(config.dim, config.task_specific_params.setdefault('cls_embedding_dimension', 512))\n",
    "\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_output = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_token_embedding = bert_output.last_hidden_state[:, 0]\n",
    "        cls_encoding = self.cls_projector(cls_token_embedding)\n",
    "        return EmbeddingOutput(embedding=cls_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "configured-point",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForClsEmbedding: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForClsEmbedding from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForClsEmbedding from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForClsEmbedding were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['cls_projector.weight', 'cls_projector.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pretrainedTst = DistilBertForClsEmbedding.from_pretrained(pretrained_model_name, task_specific_params={'cls_embedding_dimension': 100})\n",
    "pretrainedTst.distilbert = encoder.bert\n",
    "pretrainedTst.cls_projector = encoder.linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "marked-winter",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_out = pretrainedTst(**tokenizer(small_cleaned_dataset[0:10]['text'], return_tensors='pt', truncation=True, padding=True).to(device))['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "substantial-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_out = encoder(**tokenizer(small_cleaned_dataset[0:10]['text'], return_tensors='pt', truncation=True, padding=True).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "communist-policy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(torch.eq(pretrained_out, enc_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "filled-underwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_save_path = saved_model_dir.parent / f'ContextEncoder_from_{saved_model_dir.stem}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "green-estate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/cernypro/dev/source/ml4logs/models/ICT/ContextEncoder_from_2T_Eps_1_M_basic_chunked_10_Seed-42_T-len_512_C-len_512_Tr-batch_64_Ev-b_64_O-dim_100')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "informed-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrainedTst.save_pretrained(tst_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "parliamentary-holiday",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained2 = DistilBertForClsEmbedding.from_pretrained(tst_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "frequent-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained2 = pretrained2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "practical-statistics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre2_out = pretrained2(**tokenizer(small_cleaned_dataset[0:10]['text'], return_tensors='pt', truncation=True, padding=True).to(device)).embedding\n",
    "torch.all(torch.eq(pre2_out, enc_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-cooking",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
