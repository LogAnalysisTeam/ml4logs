{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cosmetic-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Union, Dict, Optional\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, AutoModel, DistilBertPreTrainedModel, AutoTokenizer\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "published-quebec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-f7d20bad4b8d075b\n",
      "Reusing dataset text (/home/cernypro/.cache/huggingface/datasets/text/default-f7d20bad4b8d075b/0.0.0/44d63bd03e7e554f16131765a251f2d8333a5fe8a73f6ea3de012dbc49443691)\n"
     ]
    }
   ],
   "source": [
    "hdfs1_dataset = load_dataset('text', data_files='../data/raw/HDFS1/HDFS.log', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "coupled-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_raw_dataset = hdfs1_dataset.select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accredited-weekend",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/cernypro/.cache/huggingface/datasets/text/default-f7d20bad4b8d075b/0.0.0/44d63bd03e7e554f16131765a251f2d8333a5fe8a73f6ea3de012dbc49443691/cache-9078a3a0732e2ad5.arrow\n"
     ]
    }
   ],
   "source": [
    "def remove_timestamp(example):\n",
    "    # need to find third occurence of a space and slice the string after it\n",
    "    # using a very non robust silly solution\n",
    "    s = example['text']\n",
    "    example['text'] = s[s.find(' ', s.find(' ', s.find(' ')+1)+1)+1:]\n",
    "    return example\n",
    "\n",
    "small_cleaned_dataset = small_raw_dataset.map(remove_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "psychological-pencil",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = \"distilbert-base-cased\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tutorial-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClsEncoderTower(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Simple model on top of a BERT like model.\n",
    "    It's a linear layer on the [CLS] token of each sentence from BERT.\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_model_name_or_path, output_encode_dimension=512):\n",
    "        super(ClsEncoderTower, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(pretrained_model_name_or_path)\n",
    "        self.linear = torch.nn.Linear(self.bert.config.dim, output_encode_dimension) # self.bert.config.dim most likely 768\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_token_embedding = bert_output[0][:, 0]\n",
    "        cls_encoding = self.linear(cls_token_embedding)\n",
    "        return cls_encoding\n",
    "    \n",
    "class OneTowerICT(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Network for the inverse close task, uses one BERT tower for creating encodings of target and context sentences (query and document as per nomenclature of original paper)\n",
    "    Uses cross entropy loss\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_model_name_or_path, output_encode_dimension=512):\n",
    "        super(OneTowerICT, self).__init__()\n",
    "        self.tower = ClsEncoderTower(pretrained_model_name_or_path, output_encode_dimension)\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    def forward(self, target, target_mask, context, context_mask, correct_class):\n",
    "        target_cls_encode = self.tower(input_ids=target, attention_mask=target_mask)\n",
    "        context_cls_encode = self.tower(input_ids=context, attention_mask=context_mask)\n",
    "        \n",
    "        logits = torch.matmul(target_cls_encode, context_cls_encode.transpose(-2, -1))\n",
    "        loss = self.loss_fn(logits, correct_class)\n",
    "        return loss, target_cls_encode, context_cls_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "reduced-chambers",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_dir = Path.cwd().parent / 'models' / '1T_Eps_2_Lines_8000000_T-len_512_C-len_512_Tr-batch_64_Ev-b_64_O-dim_512'\n",
    "saved_model_file = saved_model_dir / 'pytorch_model.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adult-casting",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(saved_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "authorized-subject",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = OneTowerICT(pretrained_model_name)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "chicken-article",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = model.tower\n",
    "encoder = encoder.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bronze-poster",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42988afb71634816b47a5b77df073396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def encode(examples, tokenizer, encoder):\n",
    "    return {'embedding': encoder(**tokenizer(examples['text'], return_tensors='pt', truncation=True, padding=True).to('cuda')).cpu().detach().numpy().tolist()}\n",
    "\n",
    "small_embedded_dataset = small_cleaned_dataset.map(encode, fn_kwargs={'tokenizer': tokenizer, 'encoder': encoder}, batched=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-brook",
   "metadata": {},
   "source": [
    "# Transforming into a huggingface PretrainedModel\n",
    "An experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "victorian-postage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.file_utils import ModelOutput\n",
    "from transformers import DistilBertModel\n",
    "\n",
    "@dataclass\n",
    "class EmbeddingOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    ModelOutput class inspired per Huggingface Transformers library conventions, may be replaced by a suitable alternative class from the library if any exists.\n",
    "    \"\"\"\n",
    "    embedding: torch.FloatTensor = None\n",
    "        \n",
    "class DistilBertForClsEmbedding(DistilBertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    DistilBertModel with a linear layer applied to [CLS] token.\n",
    "    Initialize using .from_pretrained(path_or_model_name) method\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        if config.task_specific_params is None:\n",
    "            config.task_specific_params = dict()\n",
    "\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "        self.cls_projector = torch.nn.Linear(config.dim, config.task_specific_params.setdefault('cls_embedding_dimension', 512))\n",
    "\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_output = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_token_embedding = bert_output.last_hidden_state[:, 0]\n",
    "        cls_encoding = self.cls_projector(cls_token_embedding)\n",
    "        return EmbeddingOutput(embedding=cls_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "interesting-strategy",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ICTOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    target_cls_encode: torch.FloatTensor = None\n",
    "    context_cls_encode: torch.FloatTensor = None\n",
    "    \n",
    "\n",
    "class DistilBertOneTowerICT(DistilBertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.embedding_tower = DistilBertForClsEmbedding(config)\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    def forward(self, target, target_mask, context, context_mask, correct_class):\n",
    "        target_cls_encode = self.embedding_tower(input_ids=target, attention_mask=target_mask)\n",
    "        context_cls_encode = self.embedding_tower(input_ids=context, attention_mask=context_mask)\n",
    "        \n",
    "        logits = torch.matmul(target_cls_encode, context_cls_encode.transpose(-2, -1))\n",
    "        loss = self.loss_fn(logits, correct_class)\n",
    "        return ICTOutpu(loss=loss,\n",
    "                        target_cls_encode=target_cls_encode,\n",
    "                        context_cls_encode=context_cls_encode)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "meaningful-throat",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForClsEmbedding: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForClsEmbedding from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForClsEmbedding from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForClsEmbedding were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['cls_projector.weight', 'cls_projector.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pretrainedTst = DistilBertForClsEmbedding.from_pretrained(pretrained_model_name)\n",
    "pretrainedTst.distilbert = encoder.bert\n",
    "pretrainedTst.cls_projector = encoder.linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cleared-retailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_out = pretrainedTst(**tokenizer(small_cleaned_dataset[0]['text'], return_tensors='pt', truncation=True, padding=True).to('cuda'))['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "changing-fraud",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_out = encoder(**tokenizer(small_cleaned_dataset[0:10]['text'], return_tensors='pt', truncation=True, padding=True).to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "hearing-sharing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(torch.eq(pretrained_out, enc_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "unauthorized-marble",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_save_path = saved_model_dir.parent / 'PretrainedTestDir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "breathing-ghost",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrainedTst.save_pretrained(tst_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "paperback-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained2 = DistilBertForClsEmbedding.from_pretrained(tst_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ready-maldives",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained2 = pretrained2.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "approved-clear",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre2_out = pretrained2(**tokenizer(small_cleaned_dataset[0:10]['text'], return_tensors='pt', truncation=True, padding=True).to('cuda')).embedding\n",
    "torch.all(torch.eq(pre2_out, enc_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "shared-wallace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForClsEmbedding: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForClsEmbedding from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForClsEmbedding from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForClsEmbedding were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['cls_projector.weight', 'cls_projector.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "preTstConf = DistilBertForClsEmbedding.from_pretrained(pretrained_model_name, task_specific_params={'cls_embedding_dimension': 256})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "average-transaction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embeddings(\n",
       "  (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained2.distilbert.embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
